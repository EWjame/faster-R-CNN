{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\fasterrcnn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import time\n",
    "from xml.etree import ElementTree as et\n",
    "import matplotlib.pyplot as plt\n",
    "# from config import CLASSES, RESIZE_TO, TRAIN_DIR, VALID_DIR, BATCH_SIZE\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from utils import collate_fn, get_train_transform, get_valid_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 # increase / decrease according to GPU memeory\n",
    "RESIZE_TO = 512 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 2 # number of epochs to train for\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# training images and XML files directory\n",
    "# TRAIN_DIR = './data/pascalVoc_oral/train'\n",
    "TRAIN_DIR = './data/testingcode/train'\n",
    "\n",
    "# validation images and XML files directory\n",
    "# VALID_DIR = './data/pascalVoc_oral/valid'\n",
    "VALID_DIR = './data/testingcode/valid'\n",
    "# classes: 0 index is reserved for background\n",
    "CLASSES = [\n",
    "    'background', 'oral'\n",
    "]\n",
    "NUM_CLASSES = 2\n",
    "# whether to visualize images after crearing the data loaders\n",
    "VISUALIZE_TRANSFORMED_IMAGES = False\n",
    "# location to save model and plots\n",
    "OUT_DIR = './outputs'\n",
    "if os.path.isdir(OUT_DIR) == False:\n",
    "    os.mkdir(OUT_DIR)\n",
    "SAVE_PLOTS_EPOCH = 2 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 2 # save model after these many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OralDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "        \n",
    "        # get all the image paths in sorted order\n",
    "        self.image_paths = glob.glob(f\"{self.dir_path}/*.jpg\")\n",
    "        self.all_images = [image_path.split('\\\\')[-1] for image_path in self.image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.dir_path, image_name)\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "#         image_resized = torch.tensor(image_resized,dtype=(torch.float))\n",
    "        \n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.dir_path, annot_filename)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "            \n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "            \n",
    "            # resize the bounding boxes according to the...\n",
    "            # ... desired `width`, `height`\n",
    "            xmin_final = (xmin/image_width)*self.width\n",
    "            xmax_final = (xmax/image_width)*self.width\n",
    "            ymin_final = (ymin/image_height)*self.height\n",
    "            yamx_final = (ymax/image_height)*self.height\n",
    "            \n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
    "        \n",
    "        # bounding box to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # labels to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility files\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# from config import DEVICE, CLASSES as classes\n",
    "# this class keeps track of the training and validation loss values...\n",
    "# ... and helps to get the average for each epoch as well\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number \n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def visualize_sample(image, target):\n",
    "    box = target['boxes'][0]\n",
    "    label = CLASSES[target['labels']]\n",
    "    cv2.rectangle(\n",
    "        image, \n",
    "        (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "        (0, 255, 0), 1\n",
    "    )\n",
    "    cv2.putText(\n",
    "        image, label, (int(box[0]), int(box[1]-5)), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "    )\n",
    "#     cv2.imshow('Image', image)\n",
    "#     cv2.waitKey(0)\n",
    "    _,axs = plt.subplots(1,1,figsize=(10,10))\n",
    "    axs.imshow(image)\n",
    "    axs.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OralDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES)\n",
    "valid_dataset = OralDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm_IMG_0839.jpg\n",
      "./data/testingcode/train\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.all_images[0])\n",
    "print(train_dataset.dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10\n",
      "Number of validation samples: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_SAMPLES_TO_VISUALIZE = 5\n",
    "# for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "#     image, target = train_dataset[i]\n",
    "#     visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "def create_model(num_classes):\n",
    "    \n",
    "    # load Faster RCNN pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get the number of input features \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\fasterrcnn\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\envs\\fasterrcnn\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BackboneWithFPN(\n",
       "  (body): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fpn): FeaturePyramidNetwork(\n",
       "    (inner_blocks): ModuleList(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (layer_blocks): ModuleList(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (extra_blocks): LastLevelMaxPool()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(NUM_CLASSES)\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm.tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        #To Tensor\n",
    "        images = torch.tensor(images,dtype=(torch.float))\n",
    "\n",
    "        images = [images[0].to(DEVICE)]\n",
    "        images =  [image.reshape(3,image.shape[0],image.shape[1]) for image in images]\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "#         print(images[0].shape,targets)\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list\n",
    "\n",
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm.tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = torch.tensor(images,dtype=(torch.float))\n",
    "\n",
    "        images = [images[0].to(DEVICE)]\n",
    "        images =  [image.reshape(3,image.shape[0],image.shape[1]) for image in images]\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 2\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3404: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:33<00:00, 11.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2672: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 train loss: 0.590\n",
      "Epoch #0 validation loss: 0.308\n",
      "Took 0.803 minutes for epoch 0\n",
      "\n",
      "EPOCH 2 of 2\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2371: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:31<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2250: 100%|██████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 train loss: 0.270\n",
      "Epoch #1 validation loss: 0.306\n",
      "Took 0.773 minutes for epoch 1\n",
      "SAVING MODEL COMPLETE...\n",
      "\n",
      "SAVING PLOTS COMPLETE...\n"
     ]
    }
   ],
   "source": [
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "# initialize the Averager class\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "# train and validation loss lists to store loss values of all...\n",
    "# ... iterations till ena and plot graphs for all iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'model'\n",
    "# whether to show transformed images from data loader or not\n",
    "if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "    from utils import show_tranformed_image\n",
    "    show_tranformed_image(train_loader)\n",
    "\n",
    "# initialize SaveBestModel class\n",
    "# save_best_model = SaveBestModel()\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    # reset the training and validation loss histories for the current epoch\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "    # create two subplots, one for each, training and validation\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model)\n",
    "    val_loss = validate(valid_loader, model)\n",
    "    print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    if (epoch+1) % SAVE_MODEL_EPOCH == 0: # save model after every n epochs\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')\n",
    "\n",
    "    if (epoch+1) % SAVE_PLOTS_EPOCH == 0: # save loss plots after n epochs\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "        print('SAVING PLOTS COMPLETE...')\n",
    "\n",
    "    if (epoch+1) == NUM_EPOCHS: # save loss plots and model once at the end\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"data/data.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        data = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/testingcode/train'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasterrcnn",
   "language": "python",
   "name": "fasterrcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
